{"cells":[{"cell_type":"markdown","metadata":{"id":"mcc7BjpZ0s6j"},"source":["1. training resnet18"]},{"cell_type":"code","execution_count":203,"metadata":{"executionInfo":{"elapsed":528,"status":"ok","timestamp":1732532177518,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"Gn-9_fSk0s6k"},"outputs":[],"source":["import torch\n","import torchvision\n","import numpy as np\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import ToTensor,ToPILImage\n","from torchvision.io import read_image\n","from torchvision import models\n","from PIL import Image\n","import os\n","import io\n","import pandas as pd\n","import math\n","from torch.optim import Adam\n","from torchvision.models.resnet import BasicBlock\n","\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5'"]},{"cell_type":"code","execution_count":204,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1886,"status":"ok","timestamp":1732532179885,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"KXp3Wu1h0s6l","outputId":"be597b2f-fe6c-41d6-942f-b0f4fe2e0331"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# Step 1, fine-tune VGG16 on CIFAR-10\n","\n","transform = transforms.Compose([\n","   transforms.Resize(32),\n","   transforms.RandomCrop((32, 32), padding=4),\n","   transforms.RandomHorizontalFlip(p=0.5),\n","   ToTensor(),\n","   transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n","])\n","\n","transform_val = transforms.Compose([\n","   ToTensor(),\n","   transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n","])\n","\n","\n","batch_size = 100\n","\n","trainset = torchvision.datasets.CIFAR10(root='../__data', train=True,\n","                                        download=True, transform=transform)\n","# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","#                                           shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='../__data', train=False,\n","                                        download=True, transform=transform_val)\n","# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","#                                          shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1732532179885,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"dQpBPXl_0s6m","outputId":"0f73d437-0656-4a3b-e22f-3993bdc4c3b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["500\n"]}],"source":["train_loader = torch.utils.data.DataLoader(trainset\n","    , batch_size = batch_size\n","    , shuffle = True)\n","test_loader = torch.utils.data.DataLoader(testset\n","    , batch_size = batch_size\n","    , shuffle = False)\n","n_total_step = len(train_loader)\n","print(n_total_step)"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1732532182384,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"74CuMSdi0s6m","outputId":"cb6a613f-13d9-4d57-9f66-ce9ce6128761"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}],"source":["!pip install torchmetrics\n","\n","import random\n","\n","def seed_everything(seed):\n","    torch.manual_seed(seed) #torch를 거치는 모든 난수들의 생성순서를 고정한다\n","    torch.cuda.manual_seed(seed) #cuda를 사용하는 메소드들의 난수시드는 따로 고정해줘야한다\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True #딥러닝에 특화된 CuDNN의 난수시드도 고정\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed) #numpy를 사용할 경우 고정\n","    random.seed(seed) #파이썬 자체 모듈 random 모듈의 시드 고정\n","seed_everything(42)\n","\n","\n","import torch\n","from torchmetrics import MetricCollection\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall\n","metric_collection = MetricCollection([\n","    MulticlassAccuracy(num_classes=10, average=\"micro\"),\n","    MulticlassPrecision(num_classes=10, average=\"macro\"),\n","    MulticlassRecall(num_classes=10, average=\"macro\")\n","])\n","\n","def get_metrics(device, model, test_loader):\n","  with torch.no_grad():\n","    model.to(device)\n","    model.eval()\n","    for x, y in test_loader:\n","      x = x.to(device)\n","      y = y\n","      outputs = model(x)\n","      _, predicted = outputs.max(1)\n","      metric_collection.update(predicted.cpu(), y)\n","\n","  metric_output = metric_collection.compute()\n","\n","  #print(metric_output)\n","\n","  acc, precision, recall = metric_output['MulticlassAccuracy'], metric_output['MulticlassPrecision'], metric_output['MulticlassRecall']\n","  metric_collection.reset()\n","\n","  return acc, precision, recall\n","\n","\n","def get_metrics_decore(device, model, test_loader):\n","  with torch.no_grad():\n","    model.eval()\n","    for x, y in test_loader:\n","      x = x.to(device)\n","      y = y\n","      outputs,_ = model(x)\n","      _, predicted = outputs.max(1)\n","      metric_collection.update(predicted.cpu(), y)\n","\n","  metric_output = metric_collection.compute()\n","\n","  #print(metric_output)\n","\n","  acc, precision, recall = metric_output['MulticlassAccuracy'], metric_output['MulticlassPrecision'], metric_output['MulticlassRecall']\n","  metric_collection.reset()\n","\n","  return acc, precision, recall"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2435,"status":"ok","timestamp":1732532184811,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"2V0_KjXK1C3f","outputId":"3cda9b4a-ca60-4505-dae1-5b397ee1d4a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import torch.optim as optim\n","from google.colab import drive\n","# import torch.optim as optim\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":10,"status":"ok","timestamp":1732532184811,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"Ww_JEG4C0s6n","outputId":"65f3ece8-dfaa-4b8e-fb35-e97fd92990aa"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#resnet\n","model = models.resnet18(pretrained=True)\n","\n","# Modify the first convolutional layer for CIFAR-10\n","model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","model.maxpool = nn.Identity()  # Remove maxpool as CIFAR-10 images are smaller\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","model = model.to(device)\n","\n","#print(model)"]},{"cell_type":"code","execution_count":209,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1732532184812,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"xPPpZQgK0s6n","outputId":"60ec2269-c27d-4c61-f214-38b8f1bd28f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["import time\n","\n","# Examine VGG16\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","num_epochs = 10\n","#batch_size = 40\n","learning_rate = 0.001\n","\n","criterion = nn.CrossEntropyLoss()\n","#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=1e-4)\n","#optimizer = Adam(model.parameters(), lr=learning_rate)\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","\n","#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n","\n","best_acc = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82611,"status":"ok","timestamp":1732532267417,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"YWIv9nns0s6n","outputId":"4cbd7d8f-3259-4328-942b-3bd5d08c25a4"},"outputs":[],"source":["import time\n","\n","start_time = time.time()\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    for i, (imgs, labels) in enumerate(train_loader):\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(imgs)\n","        n_corrects = (outputs.argmax(axis=1)==labels).sum().item()\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        if (i+1) % 250 == 0:\n","            accuracy, precision, recall = get_metrics(device, model, test_loader)\n","            print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {loss:.5f}, acc = {100*(n_corrects/labels.size(0)):.2f}%, val_acc = {accuracy * 100:.2f}%')\n","            if best_acc < accuracy:\n","                best_acc = accuracy\n","                print(f'save best accuracy: {best_acc}')\n","                torch.save(model, './resnet_model_best.pth')\n","\n","    # scheduler.step()\n","\n","    print()\n","\n","# 학습 종료 시간 기록\n","end_time = time.time()\n","\n","# 총 학습 시간 계산\n","total_time = end_time - start_time\n","print(f\"모델 학습 시간: {total_time // 60:.0f}분 {total_time % 60:.0f}초\")\n","\n","print(\"Resnet Training completed!!!\")\n","\n","accuracy, precision, recall = get_metrics(device, model, test_loader)\n","\n","print ('[resnet] accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n","\n","# 전체 모델 저장\n","torch.save(model, '/content/drive/MyDrive/decore_model/resnet_model.pth')"]},{"cell_type":"markdown","metadata":{"id":"4-HETQz30s6o"},"source":["2. Decore with VGG16 network"]},{"cell_type":"code","execution_count":211,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1732532267417,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"QcNxQlJ10s6o"},"outputs":[],"source":["class CustomLoss(nn.Module):\n","    def __init__(self, reward, penalty):\n","        super().__init__()\n","        self.reward = reward\n","        self.penalty = penalty # How much to penalize incorrect drop\n","\n","    def forward(self, agents_list, y_predicted, target):\n","        loss = 0.0\n","        N,_ = y_predicted.size()\n","        y_predicted = y_predicted.argmax(axis = 1)\n","        t = torch.Tensor(y_predicted.shape).to(y_predicted.device)\n","        t[y_predicted==target] = self.reward\n","        t[y_predicted!=target] = self.penalty\n","        # print(t)\n","        for a in agents_list:\n","            R = a.R*t\n","            loss += torch.mean(torch.sum(torch.log(a.Pi),dim=1)* R)\n","        return -loss # The aim is to maximize rewards\n","\n","class Agent(nn.Module):\n","    def __init__(self, id, num_channels, on_off_rate = 1):\n","        print(\"agent made\")\n","        super().__init__()\n","        self.id = id\n","        self.on_off_rate = on_off_rate\n","        self.num_channels = num_channels\n","        self.register_parameter(f'S_{id}',nn.Parameter(torch.ones(num_channels) * 6.9))\n","        _,S = next(self.named_parameters())\n","        p = nn.Sigmoid()(S.expand(1,num_channels))\n","        self.A = torch.bernoulli(p) # Actions\n","        self.Pi = torch.Tensor(self.A.shape) # policy probability\n","        self.Pi[self.A==1] = p[self.A==1]\n","        self.Pi[self.A==0] = 1 - p[self.A==0]\n","        self.R = torch.sum(abs(self.on_off_rate - self.A),dim=1) # Reward\n","        self.fixed_random = False\n","\n","    def forward(self,I):\n","        #print(\"agent forward\")\n","        N,c,h,w = I.shape\n","        _,S = next(self.named_parameters())\n","\n","        p = nn.Sigmoid()(S.expand(N,c))\n","        if self.training and not self.fixed_random:\n","            self.A = torch.bernoulli(p)\n","        elif p.shape != self.A.shape:\n","            self.A = self.A.expand(p.shape).to(S.device)\n","\n","        #self.A =self.A\n","        self.Pi = torch.Tensor(self.A.shape).to(S.device)\n","        self.Pi[self.A==1] = p[self.A==1]\n","        self.Pi[self.A==0] = 1 - p[self.A==0]\n","        self.R = torch.sum(abs(self.on_off_rate - self.A),dim=1)\n","        # print(f'agent[{self.id}] is forwarded..')\n","        return I*self.A.view(N,c,1,1).contiguous().expand(N,c,h,w)\n","\n","    def fixed_random_seed(self, flag):\n","        self.fixed_random = flag\n","\n","    def count_active_channels(self):\n","        \"\"\" 현재 활성화된 채널 수를 반환 \"\"\"\n","        return torch.sum(self.A[-1]).item()\n","\n","    def count_all_channels(self):\n","        \"\"\" 모든 채널 수를 반환 \"\"\"\n","        return self.A[-1].numel()\n","\n","    def __repr__(self):\n","        return f\"Agent({self.id}, {self.num_channels})\"\n","\n","class DECORE_basicblock(nn.Module):\n","    def __init__(self, block, agent_id_offset): #in_num, out_num, conv1_stride, conv2_stride, downsample):\n","        super().__init__()\n","        self.agent_list = []\n","        #DECORE_basicblock(a[idx].conv1.in_channels, a[idx].conv2.out_channels, a[idx].conv1.stride, a[idx].conv2.stride, a[idx].downsample)\n","        self.conv1 = block.conv1\n","        self.bn1 = block.bn1\n","        self.relu = block.relu\n","        self.agent1 = Agent(agent_id_offset, block.conv1.out_channels)\n","        self.conv2 = block.conv2\n","        self.bn2 = block.bn2\n","\n","        self.downsample = block.downsample\n","        # setting bypass conv for residual input\n","        # if self.downsample is None:\n","        #     self.conv_bypass = nn.Conv2d(block.conv1.in_channels, block.conv2.out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n","        #     # self.conv_bypass.weight.copy_(torch.eye(self.conv_bypass.in_channels).view(self.conv_bypass.out_channels, self.conv_bypass.in_channels, 1, 1))\n","        #     # self.conv_bypass = self.conv_bypass.weight.copy_(torch.eye(self.conv_bypass.in_channels).view(self.conv_bypass.out_channels, self.conv_bypass.in_channels, 1, 1))\n","        # else:\n","        #     self.conv_bypass = nn.Conv2d(self.downsample[0].out_channels, block.conv2.out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n","\n","        # with torch.no_grad():\n","        #     self.conv_bypass.weight.copy_(torch.eye(self.conv_bypass.in_channels).view(self.conv_bypass.out_channels, self.conv_bypass.in_channels, 1, 1))\n","        # # with torch.no_grad():\n","        # #     self.conv_bypass.weight.fill_(1.0)\n","        # self.conv_bypass.requires_grad = False\n","\n","        # self.agent2 = Agent(agent_id_offset + 1, block.conv2.out_channels)\n","        self.agent_list.append(self.agent1)\n","        # self.agent_list.append(self.agent2)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        identity = x\n","\n","        # print('**********************************')\n","        # print(f'input x shape: {x.shape}')\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.agent1(out)\n","        # print(f'after conv1 out shape: {out.shape}')\n","\n","        #out = self.agent1(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        # out = self.agent2(out)\n","        # self.agent2.fixed_random_seed(True)\n","        # print(f'decore_conv2_agent_A: {self.agent2.A[0]}')\n","\n","        # print(f'after conv2 out shape: {out.shape}')\n","\n","        if self.downsample is not None:\n","            # print(f'before downsample x shape: {x.shape}')\n","            identity = self.downsample(x)\n","            # print(f'after downsample identity shape: {identity.shape}')\n","\n","        # print(f'before conv_bypass identity shape: {identity.shape}')\n","        # identity = self.conv_bypass(identity)\n","        # print(f'after conv_bypass identity shape: {identity.shape}')\n","        # print(f'before agent_identity')\n","        # print(identity[0][0])\n","        # identity = self.conv_bypass(identity)\n","        # print(f'after agent_identity')\n","        # print(identity[0][0])\n","        # identity = self.agent2(identity)\n","        # self.agent2.fixed_random_seed(False)\n","        # print(f'conv_bypass weight')\n","        # print(self.conv_bypass.weight)\n","        # print(f'decore_indentity_agent_A: {self.agent2.A[0]}')\n","\n","        out += identity\n","        out = self.relu(out)\n","        # print(f'last out shape: {out.shape}')\n","\n","        return out\n","\n","    def count_all_channels(self):\n","        return self.agent1.count_all_channels() + self.agent2.count_all_channels()\n","\n","\n","class DECOR_RESNET(nn.Module):\n","    def __init__(self, Model):\n","        super().__init__()\n","        import copy\n","        self.target_model = copy.deepcopy(Model)\n","        self.agents_list = [] # To keep track of all newly added agents\n","        self.block_list = []\n","        self.parse_model()\n","        self.agents = nn.ModuleList(self.agents_list)\n","        # self.blocks = nn.ModuleList(self.block_list)\n","\n","    def parse_model(self):\n","        \"\"\" This method parse the target model and append agents after the activation function of each convolution, the dictionary of the\n","            target model should be defined for easy parsing \"\"\"\n","        n = ''\n","        #modules = torch.nn.Sequential()\n","\n","        for i in self.target_model.state_dict().keys():\n","            if n != i.split('.')[0]:\n","                n = i.split('.')[0]\n","                a = getattr(self.target_model, f'{n}')\n","                if hasattr(a, '__iter__'):\n","                    modules = torch.nn.Sequential()\n","                    for idx in range(0,len(a)):\n","                        from torchvision.models.resnet import BasicBlock\n","                        if isinstance(a[idx], BasicBlock):\n","                            block = DECORE_basicblock(a[idx], len(self.agents_list)) #DECORE_basicblock(a[idx].conv1.in_channels, a[idx].conv2.out_channels, a[idx].conv1.stride, a[idx].conv2.stride, a[idx].downsample)\n","                            modules.add_module(f'{idx}', block)\n","                            self.block_list.append(block)\n","                            for agnt in block.agent_list:\n","                                self.agents_list.append(agnt)\n","\n","                    a = modules\n","                    setattr(self.target_model, f'{n}',a)\n","\n","    def forward(self,I):\n","        return self.target_model(I)\n","\n","    def frozen_agent(is_fronzen=True):\n","        for agent in self.agents_list:\n","            for name, param in agent.named_parameters():\n","                param.requires_grad = not is_fronzen\n","\n","    def count_total_active_channels(self):\n","        \"\"\" 모든 에이전트에서 활성화된 채널 수의 총합을 반환 \"\"\"\n","        total_active_channels = sum(agent.count_active_channels() for agent in self.agents)\n","        return total_active_channels\n","\n","    def count_total_all_channels(self):\n","        \"\"\" 모든 에이전트에서 모든 채널 수의 총합을 반환 \"\"\"\n","        #print('agent총 수 : ', len(self.agents))\n","        total_channels = sum(agent.count_all_channels() for agent in self.agents)\n","        return total_channels\n","\n","    def print_active_channel(self):\n","        \"\"\"각 에이전트의 활성화된 채널 수 출력\"\"\"\n","        for index, agent in enumerate(self.agents, 1):\n","            #print(f\"Agent {index} 상태 - A: {agent.A[0]}\")  # A 벡터 출력\n","            active_channels = agent.count_active_channels()\n","            print(f\"CNN Layer #{index} Active Channels: {active_channels} / All Channels: {agent.count_all_channels()}\")\n"]},{"cell_type":"code","execution_count":212,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10213,"status":"ok","timestamp":1732532277599,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"FUVe9TqX0s6o","outputId":"dcf3661f-d7a5-4054-9723-f199b985ca5a"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-212-1282e41fbed7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load('/content/drive/MyDrive/decore_model/resnet_model.pth')\n"]},{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): Identity()\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n","[before resnet] decore accuracy :  tensor(0.7619) , precision :  tensor(0.7661) , recall :  tensor(0.7619)\n"]}],"source":["#model = torch.load('./resnet18_bestacc.pth')\n","model = torch.load('/content/drive/MyDrive/decore_model/resnet_model.pth')\n","print(model)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","accuracy, precision, recall = get_metrics(device, model, test_loader)\n","model.train()\n","print ('[before resnet] decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)"]},{"cell_type":"code","execution_count":213,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12025,"status":"ok","timestamp":1732532289605,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"1y89O6R90s6p","outputId":"b7a65f0e-95ce-485d-86ae-afdb6b7e8e58"},"outputs":[{"name":"stdout","output_type":"stream","text":["agent made\n","agent made\n","agent made\n","agent made\n","agent made\n","agent made\n","agent made\n","agent made\n","DECOR_RESNET(\n","  (target_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): Identity()\n","    (layer1): Sequential(\n","      (0): DECORE_basicblock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(0, 64)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): DECORE_basicblock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(1, 64)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): DECORE_basicblock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(2, 128)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): DECORE_basicblock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(3, 128)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): DECORE_basicblock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(4, 256)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): DECORE_basicblock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(5, 256)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): DECORE_basicblock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(6, 512)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): DECORE_basicblock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (agent1): Agent(7, 512)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=10, bias=True)\n","  )\n","  (agents): ModuleList(\n","    (0): Agent(0, 64)\n","    (1): Agent(1, 64)\n","    (2): Agent(2, 128)\n","    (3): Agent(3, 128)\n","    (4): Agent(4, 256)\n","    (5): Agent(5, 256)\n","    (6): Agent(6, 512)\n","    (7): Agent(7, 512)\n","  )\n",")\n","[before resnet] decore accuracy :  tensor(0.7649) , precision :  tensor(0.7664) , recall :  tensor(0.7649)\n"]}],"source":["net = DECOR_RESNET(model)\n","net = net.to(device)\n","print(net)\n","\n","accuracy, precision, recall = get_metrics(device, net, test_loader)\n","model.train()\n","print ('[before resnet] decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)"]},{"cell_type":"code","execution_count":214,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1732532289605,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"Q_pbnWa80s6p","outputId":"eebcce31-af82-4e67-cb91-505c3443174b"},"outputs":[{"name":"stdout","output_type":"stream","text":["reward: 1, penalty: -30\n"]}],"source":["id = 0\n","param = []\n","\n","for n,p in net.named_parameters():\n","    if n.endswith(f\".S_{id}\") and p.requires_grad:\n","        param.append(p)\n","        id += 1\n","\n","#optimizer = Adam(param, lr=lr)\n","\n","# optimizer = optim.Adam(net.agents.parameters(), lr=lr) # 이거로 변경해보기\n","optimizer = optim.Adam(net.agents.parameters(), lr=0.01) # 이거로 변경해보기\n","#optimizer = torch.optim.SGD(net.agents.parameters(), lr = lr, momentum=0.9,weight_decay=5e-4)\n","# optimizer = torch.optim.SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=5e-4)\n","# optimizer = torch.optim.SGD(net.agents.parameters(), lr = lr)\n","#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,40,60,80], gamma=0.5)\n","criterion_decore = CustomLoss(1, -30)\n","print(f'reward: {criterion_decore.reward}, penalty: {criterion_decore.penalty}')\n","\n","base_lr = 0.00001\n","base_criterion = nn.CrossEntropyLoss()\n","# base_optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9,weight_decay=5e-4)\n","base_optimizer = optim.Adam(net.parameters(), lr=base_lr)\n","\n","import gc\n","import time\n","\n","torch.set_printoptions(edgeitems=torch.inf)\n","gc.collect()\n","torch.cuda.empty_cache()\n","best_acc = 0\n","best_comp_rate = 1\n","best_score = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"elapsed":5856,"status":"ok","timestamp":1732532295453,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"ar5tY1nJwBZb","outputId":"4489f402-0ed4-401b-b2cb-3fd841877eb8"},"outputs":[],"source":["#wandb 로깅\n","!pip install wandb\n","!wandb login\n","import wandb\n","import numpy as np\n","from datetime import datetime\n","\n","num_epochs = 5\n","\n","current_date = datetime.now().strftime(\"%Y-%m-%d\")  # 현재 날짜를 기반으로 run_name 생성(형식: YYYY-MM-DD)\n","run_name = f\"DECORE-PRE_RESNET-{current_date}\"\n","\n","wandb.init(\n","    project=\"DECORE-PRE_RESNET\",\n","    name=run_name,\n","    config={\n","        \"architecture\": \"ResNet18\",\n","        \"dataset\": \"CIFAR10\",\n","        \"epochs\": num_epochs,\n","        \"batch_size\": batch_size,\n","        \"learning_rate\": base_lr,\n","        \"reward\": criterion_decore.reward,\n","        \"penalty\": criterion_decore.penalty\n","    }\n",")\n","\n","def log_layer_metrics(net, epoch):\n","    \"\"\"Log compression rates for each layer\"\"\"\n","    for idx, agent in enumerate(net.agents, 1):\n","        active_channels = agent.count_active_channels()\n","        total_channels = agent.count_all_channels()\n","        compression_rate = active_channels / total_channels\n","\n","        # Add row to the table\n","        layer_compression_table.add_data(epoch, f\"Layer {idx}\", active_channels, total_channels, compression_rate)\n","\n","        # Log individual layer metrics\n","        wandb.log({\n","            f\"layer_{idx}/compression_rate\": compression_rate,\n","            f\"layer_{idx}/active_channels\": active_channels\n","        })\n","\n","def log_training_metrics(epoch, step, total_loss, running_corrects, total_samples,\n","                        test_accuracy, test_precision, test_recall, total_active_channel,\n","                        total_all_channel):\n","    \"\"\"Log training and testing metrics\"\"\"\n","    train_accuracy = running_corrects / total_samples\n","    compression_rate = total_active_channel / total_all_channel\n","\n","    wandb.log({\n","        \"epoch\": epoch,\n","        \"step\": step,\n","        \"loss\": total_loss,\n","        \"train_accuracy\": train_accuracy,\n","        \"test_accuracy\": test_accuracy,\n","        \"test_precision\": test_precision,\n","        \"test_recall\": test_recall,\n","        \"overall_compression_rate\": compression_rate,\n","        \"active_channels\": total_active_channel,\n","        \"total_channels\": total_all_channel\n","    })"]},{"cell_type":"code","execution_count":216,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c4bc954e7b604fffaf12c1522bdf312e","69fd37a6b4854a5590db6777dbc2c66d","04288712d09a45d88eb6f989cd285513","a7f8bbcc79784612a311b295353da9d3","6f436fc1aa364e068cbef5c859f57320","29c98210939d4662ab597e8de2ef61f5","dd8aef59a6da470baab5954d110db8f9","7b8a588eda6d4edaab7ebacba9ceb918"]},"executionInfo":{"elapsed":860229,"status":"ok","timestamp":1732533156180,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"rQeapwP40s6p","outputId":"e021fe90-0c13-46f5-8106-df1a45436e96"},"outputs":[{"name":"stdout","output_type":"stream","text":["decore training start\n","epoch 1/5, step: 250/500: loss = -42999.37891, train_acc = 70.89%, test_acc = 72.14%, channel = 99.88%\n","[64, 64, 128, 128, 256, 254, 511, 511]\n","epoch 1/5, step: 500/500: loss = -77882.78906, train_acc = 70.97%, test_acc = 72.29%, channel = 99.90%\n","[64, 64, 128, 128, 256, 256, 511, 512]\n","epoch 2/5, step: 250/500: loss = -27537.19336, train_acc = 71.06%, test_acc = 72.55%, channel = 99.92%\n","[64, 64, 128, 128, 256, 255, 512, 512]\n","epoch 2/5, step: 500/500: loss = -49249.40625, train_acc = 71.21%, test_acc = 72.35%, channel = 99.93%\n","[64, 64, 128, 128, 256, 255, 512, 512]\n","epoch 3/5, step: 250/500: loss = -18032.25586, train_acc = 71.22%, test_acc = 72.01%, channel = 99.95%\n","[64, 64, 128, 128, 256, 256, 512, 512]\n","epoch 3/5, step: 500/500: loss = -33048.63281, train_acc = 71.21%, test_acc = 72.17%, channel = 99.95%\n","[64, 64, 128, 128, 255, 256, 511, 511]\n","epoch 4/5, step: 250/500: loss = -13285.19336, train_acc = 71.08%, test_acc = 72.24%, channel = 99.96%\n","[64, 64, 128, 128, 256, 256, 511, 512]\n","epoch 4/5, step: 500/500: loss = -24711.01172, train_acc = 71.23%, test_acc = 72.28%, channel = 99.96%\n","[64, 64, 128, 128, 256, 256, 512, 512]\n","epoch 5/5, step: 250/500: loss = -10467.70703, train_acc = 71.44%, test_acc = 72.12%, channel = 99.97%\n","[64, 64, 128, 128, 256, 256, 511, 512]\n","epoch 5/5, step: 500/500: loss = -19550.31641, train_acc = 71.26%, test_acc = 72.04%, channel = 99.97%\n","[64, 64, 128, 128, 256, 256, 512, 512]\n","[base_model] epoch 5/5, step: 250/500: loss = 0.71375, acc = 73.00%, test_acc = 78.09%, score = 0.7811123728752136\n","save best score model: 0.7811 (comp rate: 0.9997, acc: 0.7809)\n","[base_model] epoch 5/5, step: 500/500: loss = 0.63497, acc = 80.00%, test_acc = 80.66%, score = 0.8068193197250366\n","save best score model: 0.8068 (comp rate: 0.9997, acc: 0.8066)\n","[base_model] epoch 5/5, step: 250/500: loss = 0.55587, acc = 78.00%, test_acc = 82.15%, score = 0.8217234015464783\n","save best score model: 0.8217 (comp rate: 0.9997, acc: 0.8215)\n","[base_model] epoch 5/5, step: 500/500: loss = 0.59291, acc = 80.00%, test_acc = 82.58%, score = 0.8260245323181152\n","save best score model: 0.8260 (comp rate: 0.9997, acc: 0.8258)\n","[base_model] epoch 5/5, step: 250/500: loss = 0.42441, acc = 80.00%, test_acc = 84.16%, score = 0.8418288826942444\n","save best score model: 0.8418 (comp rate: 0.9997, acc: 0.8416)\n","[base_model] epoch 5/5, step: 500/500: loss = 0.40380, acc = 89.00%, test_acc = 84.35%, score = 0.8437293767929077\n","save best score model: 0.8437 (comp rate: 0.9997, acc: 0.8435)\n","[base_model] epoch 5/5, step: 250/500: loss = 0.39573, acc = 87.00%, test_acc = 84.90%, score = 0.8492308259010315\n","save best score model: 0.8492 (comp rate: 0.9997, acc: 0.8490)\n","[base_model] epoch 5/5, step: 500/500: loss = 0.49233, acc = 83.00%, test_acc = 85.22%, score = 0.8524317145347595\n","save best score model: 0.8524 (comp rate: 0.9997, acc: 0.8522)\n","[base_model] epoch 5/5, step: 250/500: loss = 0.33036, acc = 90.00%, test_acc = 85.87%, score = 0.8589335083961487\n","save best score model: 0.8589 (comp rate: 0.9997, acc: 0.8587)\n","[base_model] epoch 5/5, step: 500/500: loss = 0.51492, acc = 80.00%, test_acc = 86.06%, score = 0.860834002494812\n","save best score model: 0.8608 (comp rate: 0.9997, acc: 0.8606)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4bc954e7b604fffaf12c1522bdf312e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compression/active_channels</td><td>▁▄▆▇█</td></tr><tr><td>compression/rate</td><td>▁▄▆▇█</td></tr><tr><td>compression/total_channels</td><td>▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆██████</td></tr><tr><td>finetune/epoch</td><td>▁▃▅▆█</td></tr><tr><td>finetune/loss</td><td>█▅▃▂▁</td></tr><tr><td>finetune/score</td><td>▁▃▆▇█</td></tr><tr><td>finetune/test_accuracy</td><td>▁▃▆▇█</td></tr><tr><td>finetune/train_accuracy</td><td>▁▄▆▇█</td></tr><tr><td>test/accuracy</td><td>▇█▄▆▁</td></tr><tr><td>test/precision</td><td>▆▆▃█▁</td></tr><tr><td>test/recall</td><td>▇█▄▆▁</td></tr><tr><td>train/accuracy</td><td>▁▇▇▇█</td></tr><tr><td>train/loss</td><td>▁▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compression/active_channels</td><td>959739</td></tr><tr><td>compression/rate</td><td>99.97281</td></tr><tr><td>compression/total_channels</td><td>960000</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>finetune/epoch</td><td>5</td></tr><tr><td>finetune/loss</td><td>0.39746</td></tr><tr><td>finetune/score</td><td>0.86083</td></tr><tr><td>finetune/test_accuracy</td><td>86.06</td></tr><tr><td>finetune/train_accuracy</td><td>86.146</td></tr><tr><td>test/accuracy</td><td>72.04</td></tr><tr><td>test/precision</td><td>72.81286</td></tr><tr><td>test/recall</td><td>72.04</td></tr><tr><td>train/accuracy</td><td>71.262</td></tr><tr><td>train/loss</td><td>-39.10062</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">DECORE-PRE_RESNET-2024-11-25</strong> at: <a href='https://wandb.ai/dlwldk-bc-card/DECORE-PRE_RESNET/runs/hg6g1yos' target=\"_blank\">https://wandb.ai/dlwldk-bc-card/DECORE-PRE_RESNET/runs/hg6g1yos</a><br/> View project at: <a href='https://wandb.ai/dlwldk-bc-card/DECORE-PRE_RESNET' target=\"_blank\">https://wandb.ai/dlwldk-bc-card/DECORE-PRE_RESNET</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241125_105813-hg6g1yos/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["decore 모델 학습 시간: 14분 15초\n","총 채널 수 :  1920\n","활성화된 채널 수 :  1920.0\n","decore accuracy :  tensor(0.8606) , precision :  tensor(0.8600) , recall :  tensor(0.8606)\n","Layer 0 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] / (64,)\n","Layer 1 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] / (64,)\n","Layer 2 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1.] / (128,)\n","Layer 3 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1.] / (128,)\n","Layer 4 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] / (256,)\n","Layer 5 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] / (256,)\n","Layer 6 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1.] / (512,)\n","Layer 7 active channels: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1.] / (512,)\n"]}],"source":["print(\"decore training start\")\n","# 학습 시작 시간 기록\n","start_time = time.time()\n","net.train()\n","\n","# wandb table 만들기\n","epoch_table = wandb.Table(columns=[\"epoch\", \"train_loss\", \"train_accuracy\", \"test_accuracy\",\n","                                 \"compression_rate\", \"active_channels\", \"total_channels\"])\n","\n","metrics_table = wandb.Table(\n","    columns=[\"epoch\", \"train_loss\", \"train_accuracy\", \"test_accuracy\",\n","             \"compression_rate\", \"active_channels\", \"total_channels\"])\n","\n","layer_table = wandb.Table(\n","    columns=[\"epoch\"] + [f\"layer_{i}_compression\" for i in range(len(net.agents_list))])\n","\n","for epoch in range(num_epochs):\n","    running_corrects = 0 # Accuracy calculation\n","    total_samples = 0 # Accuracy calculation\n","    total_loss = 0\n","    total_all_channel = 0\n","    total_active_channel = 0\n","\n","    # epoch별\n","    epoch_total_loss = 0\n","    epoch_running_corrects = 0\n","    epoch_total_samples = 0\n","    epoch_all_channel = 0\n","    epoch_active_channel = 0\n","\n","    #train_loss = 0.0\n","    for i, (imgs , labels) in enumerate(train_loader):\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","        labels_hat = net.target_model(imgs)\n","        loss_value = criterion_decore(net.agents_list, labels_hat, labels)\n","\n","        _, preds = torch.max(labels_hat, 1)\n","        running_corrects += torch.sum(preds == labels).item()\n","        total_samples += labels.size(0)\n","\n","        total_loss += loss_value\n","        total_all_channel += net.count_total_all_channels()\n","        total_active_channel += net.count_total_active_channels()\n","        curr_comp_rate = total_active_channel / total_all_channel\n","\n","        # epoch별\n","        epoch_running_corrects += torch.sum(preds == labels).item()\n","        epoch_total_samples += labels.size(0)\n","        epoch_total_loss += loss_value.item()\n","        epoch_all_channel += net.count_total_all_channels()\n","        epoch_active_channel += net.count_total_active_channels()\n","\n","        if (i + 1) % (n_total_step / 2) == 0:\n","            accuracy, precision, recall = get_metrics(device, net, test_loader)\n","            # print ('decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n","            net.train()\n","\n","            print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {total_loss:.5f}, train_acc = {(100 * running_corrects / total_samples):.2f}%, test_acc = {accuracy * 100:.2f}%, channel = { curr_comp_rate * 100:.2f}%')\n","            # print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {total_loss:.5f}, train_acc = {(100 * running_corrects / total_samples):.2f}%, channel = { total_active_channel / total_all_channel * 100:.2f}%, lr ={optimizer.param_groups[0][\"lr\"]}')\n","            active_cnt_list = []\n","            for agnt in net.agents_list:\n","                active_cnt_list.append(int(agnt.count_active_channels()))\n","            print(active_cnt_list)\n","            if 0:\n","                for agent in net.agents_list:\n","                    rounded_Pi = torch.round(agent.Pi * 10000) / 10000\n","                    unique_values, counts = torch.unique(rounded_Pi, return_counts=True)\n","                    #print(agent.Pi[agent.Pi != 0.999])\n","                    for value, count in zip(unique_values, counts):\n","                        print(f\"Value: {value.item():.5f}, Count: {count.item()}\")\n","                    break\n","                total_loss = 0\n","\n","        #train_loss += loss_value.detach().cpu().item() / len(train_loader)\n","        optimizer.zero_grad()\n","        #loss_value = loss_value.detach()\n","        loss_value.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","\n","\n","    # epoch 종료 후 metrics 계산 및 기록\n","    epoch_avg_loss = epoch_total_loss / len(train_loader)\n","    epoch_accuracy = epoch_running_corrects / epoch_total_samples\n","    epoch_comp_rate = epoch_active_channel / epoch_all_channel\n","\n","    # 테스트 metrics 계산\n","    final_accuracy, final_precision, final_recall = get_metrics(device, net, test_loader)\n","    net.train()\n","\n","    # epoch table에 데이터 추가\n","    metrics_table.add_data(\n","        epoch + 1,\n","        float(epoch_avg_loss),  # tensor를 float로 변환\n","        float(epoch_accuracy * 100),\n","        float(final_accuracy * 100),\n","        float(epoch_comp_rate * 100),\n","        int(epoch_active_channel),\n","        int(epoch_all_channel)\n","    )\n","\n","    # 레이어별 압축률 계산 및 기록\n","    layer_compressions = [epoch + 1]\n","    for agent in net.agents_list:\n","        active_channels = agent.count_active_channels()\n","        total_channels = agent.Pi.shape[0]\n","        compression_rate = (active_channels / total_channels) * 100\n","        layer_compressions.append(compression_rate)\n","    layer_compression_table.add_data(*layer_compressions)\n","\n","    # wandb에 epoch 단위 metrics 기록\n","    wandb.log({\n","        \"epoch\": epoch + 1,\n","        \"train/loss\": epoch_avg_loss,\n","        \"train/accuracy\": epoch_accuracy * 100,\n","        \"test/accuracy\": final_accuracy * 100,\n","        \"test/precision\": final_precision * 100,\n","        \"test/recall\": final_recall * 100,\n","        \"compression/rate\": epoch_comp_rate * 100,\n","        \"compression/active_channels\": epoch_active_channel,\n","        \"compression/total_channels\": epoch_all_channel\n","    })\n","\n","    if 1: # fine-tuning\n","        if (epoch + 1) % 5 == 0:\n","            # base model training\n","            for agent in net.agents_list:\n","                for name, param in agent.named_parameters():\n","                    param.requires_grad = False\n","\n","\n","            ft_metrics = {\n","                \"loss\": [],\n","                \"train_acc\": [],\n","                \"test_acc\": [],\n","                \"score\": []\n","            }\n","\n","            for ft_epoch in range(5):\n","                ft_running_loss = 0\n","                ft_corrects = 0\n","                ft_total = 0\n","\n","                for i, (imgs , labels) in enumerate(train_loader):\n","                    imgs = imgs.to(device)\n","                    labels = labels.to(device)\n","\n","                    labels_hat = net.target_model(imgs)\n","                    n_corrects = (labels_hat.argmax(axis=1)==labels).sum().item()\n","                    ft_corrects += n_corrects\n","                    ft_total += labels.size(0)\n","\n","                    base_loss_value = base_criterion(labels_hat, labels)\n","                    base_loss_value.backward()\n","                    base_optimizer.step()\n","                    base_optimizer.zero_grad()\n","                    ft_running_loss += base_loss_value.item()\n","\n","                    if (i+1) % (n_total_step / 2) == 0:\n","                        accuracy, precision, recall = get_metrics(device, net, test_loader)\n","                        net.train()\n","                        score = accuracy / curr_comp_rate\n","                        print(f'[base_model] epoch {epoch+1}/{5}, step: {i+1}/{n_total_step}: loss = {base_loss_value:.5f}, acc = {100*(n_corrects/labels.size(0)):.2f}%, test_acc = {accuracy * 100:.2f}%, score = {score}')\n","                        if best_score < score:\n","                            best_score = score\n","                            print(f'save best score model: {best_score:.4f} (comp rate: {curr_comp_rate:.4f}, acc: {accuracy:.4f})')\n","                            torch.save(net, f'./decore_resnet_best.pth')\n","\n","                ft_epoch_loss = ft_running_loss / len(train_loader)\n","                ft_train_acc = ft_corrects / ft_total\n","                ft_test_acc, _, _ = get_metrics(device, net, test_loader)\n","                ft_score = ft_test_acc / curr_comp_rate\n","\n","                # wandb에 fine-tuning metrics 기록\n","                wandb.log({\n","                    \"epoch\": epoch + 1,\n","                    \"finetune/epoch\": ft_epoch + 1,\n","                    \"finetune/loss\": ft_epoch_loss,\n","                    \"finetune/train_accuracy\": ft_train_acc * 100,\n","                    \"finetune/test_accuracy\": ft_test_acc * 100,\n","                    \"finetune/score\": ft_score\n","                })\n","\n","            for agent in net.agents_list:\n","                for name, param in agent.named_parameters():\n","                    param.requires_grad = True\n","\n","    if epoch % 100 == 0:\n","        # Google Drive에 모델 가중치 저장\n","        torch.save(net.state_dict(), f'./decore_resnet_model_weights_{epoch}.pth')\n","        # 전체 모델 저장\n","        torch.save(net, f'./decore_resnet_model_{epoch}.pth')\n","\n","# wandb table 기록\n","wandb.log({\n","    \"epoch_metrics\": epoch_table,\n","    \"layer_compression_history\": layer_compression_table\n","})\n","\n","# wandb end\n","wandb.finish()\n","\n","# 학습 종료 시간 기록\n","end_time = time.time()\n","\n","# 총 학습 시간 계산\n","total_time = end_time - start_time\n","print(f\"decore 모델 학습 시간: {total_time // 60:.0f}분 {total_time % 60:.0f}초\")\n","print(\"총 채널 수 : \", net.count_total_all_channels())\n","print(\"활성화된 채널 수 : \",net.count_total_active_channels())\n","\n","net.eval()\n","accuracy, precision, recall = get_metrics(device, net.target_model, test_loader)\n","\n","print ('decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n","\n","# DECORE 모델에서 활성화된 채널 정보 가져오기\n","active_channels_list = []\n","for agent in net.agents:\n","    agent.eval()\n","    # 각 agent에서 활성화된 채널의 인덱스 가져오기 (1차원으로 추출)\n","    active_channels_list.append(agent.A[-1].detach().cpu().numpy())\n","\n","for i, active_channels in enumerate(active_channels_list):\n","    print(f\"Layer {i} active channels: {active_channels} / {active_channels.shape}\")\n","\n","# Google Drive에 모델 가중치 저장\n","torch.save(net.state_dict(), './decore_resnet_model_final_weights.pth')\n","\n","# 전체 모델 저장\n","torch.save(net, './decore_resnet_model_final.pth')\n","\n","\n","# 전체 모델 저장\n","torch.save(net, '/content/drive/MyDrive/decore_model/decore_resnet_model.pth')\n"]},{"cell_type":"markdown","metadata":{"id":"5ZrHu-Pj0s6q"},"source":["3. structured pruning"]},{"cell_type":"code","execution_count":217,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1732533156181,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"HaY5oKj00s6q"},"outputs":[],"source":["class NewBasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None, index =0):\n","        super(NewBasicBlock, self).__init__()\n","        # 첫 번째 Conv 레이어\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        # 두 번째 Conv 레이어\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # downsample이 필요한 경우 (입력과 출력의 크기가 다를 경우 처리)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.index = index\n","\n","        if self.downsample is None:\n","            self.conv_bypass = nn.Conv2d(self.conv1.in_channels, self.conv2.out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n","        else:\n","            self.conv_bypass = nn.Conv2d(self.downsample[0].out_channels, self.conv2.out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n","\n","        with torch.no_grad():\n","            in_channels = self.conv_bypass.in_channels\n","            out_channels = self.conv_bypass.out_channels\n","            # Initialize weight with zeros\n","            weight = torch.zeros(out_channels, in_channels, 1, 1)\n","            # Fill the matching channels with an identity matrix\n","            idx = min(in_channels, out_channels)\n","            weight[:idx, :idx, 0, 0] = torch.eye(idx)\n","            self.conv_bypass.weight.copy_(weight)\n","        self.conv_bypass.requires_grad = False\n","\n","        # 디버깅을 위한 초기화 상태 저장\n","        self._debug_info = {\n","            \"conv_bypass_weight\": self.conv_bypass.weight.clone(),\n","        }\n","\n","    def forward(self, x):\n","        #print(\"layer index : \", self.index)\n","        identity = x\n","        #agent conv의 inchannel수\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # downsample 경로를 통해 크기를 맞춤\n","        if self.downsample is not None:\n","            #print(\"down sample is not none\")\n","            identity = self.downsample(x)\n","\n","        # Skip connection\n","        identity = self.conv_bypass(identity)\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","    def debug(self):\n","      \"\"\"디버깅 정보를 출력하는 함수\"\"\"\n","      print(f\"Debugging Block {self.index}:\")\n","\n","      # 초기화된 weight와 현재 weight의 차이 계산\n","      diff = self.conv_bypass.weight - self._debug_info[\"conv_bypass_weight\"]\n","      nonzero_diff = diff.abs() > 1e-8  # 차이가 있는 위치 확인\n","\n","      # 차이가 있는 요소 출력\n","      if torch.any(nonzero_diff):\n","          print(\"Bypass weight 차이 (non-zero elements):\")\n","          diff_indices = nonzero_diff.nonzero(as_tuple=True)  # 차이가 있는 위치 인덱스\n","          for i in range(len(diff_indices[0])):\n","              print(f\"Index {tuple(d[i] for d in diff_indices)}: {diff[diff_indices][i].item()}\")\n","          print(\"오류남 ==> Bypass weights 바뀜\")\n","      else:\n","          print(\"정상임 ==> Bypass weights 일정함\")\n","\n","\n","class ModifiedResNet(nn.Module):\n","    def __init__(self, resnet18, active_channels_list):\n","        super(ModifiedResNet, self).__init__()\n","        self.features = nn.ModuleList()\n","        self.active_channels_list = active_channels_list\n","        prev_out_channels = 3\n","        conv_layer_count = 0\n","\n","        for i, layer in enumerate(resnet18.children()):\n","            if isinstance(layer, nn.Sequential):\n","                new_seq_layers = []\n","                for j, sub_layer in enumerate(layer):\n","                    if isinstance(sub_layer, DECORE_basicblock):\n","                        active_out_channels = torch.tensor(self.active_channels_list[conv_layer_count], dtype=torch.bool)\n","                        out_indices = active_out_channels.nonzero().squeeze(1)\n","\n","                        if conv_layer_count > 0:\n","                            active_in_channels = torch.tensor(self.active_channels_list[conv_layer_count - 1], dtype=torch.bool)\n","                            in_indices = active_in_channels.nonzero().squeeze(1)\n","                        else:\n","                            active_in_channels = torch.ones(prev_out_channels, dtype=torch.bool)\n","                            in_indices = active_in_channels.nonzero().squeeze(1)\n","\n","                        # Downsample 경로 수정\n","                        if sub_layer.downsample is not None:\n","                            downsample = nn.Sequential()\n","                            before_conv1 = list(sub_layer.downsample.children())[0]\n","                            before_bn1 = list(sub_layer.downsample.children())[1]\n","\n","                            # Downsample conv는 원본 채널 수 유지\n","                            conv1 = before_conv1\n","                            downsample.add_module('0', conv1)\n","                            downsample.add_module('1', before_bn1)\n","                        else:\n","                            downsample = None\n","\n","                        # Create new block with original channel dimensions\n","                        new_block = NewBasicBlock(sub_layer.conv1.in_channels,\n","                                                sub_layer.conv2.out_channels,\n","                                                stride=sub_layer.conv1.stride,\n","                                                downsample=downsample,\n","                                                index=j)\n","                        new_block.debug()\n","\n","                        # Conv1 수정 (출력 채널만 pruning)\n","                        if sub_layer.conv1.bias is None:\n","                            bias_flag = False\n","                        else:\n","                            bias_flag = True\n","\n","                        conv1 = nn.Conv2d(sub_layer.conv1.in_channels,\n","                                        len(out_indices),\n","                                        kernel_size=sub_layer.conv1.kernel_size,\n","                                        stride=sub_layer.conv1.stride,\n","                                        padding=sub_layer.conv1.padding,\n","                                        bias=bias_flag)\n","\n","                        copied_weights = sub_layer.conv1.weight.data[out_indices]\n","                        conv1.weight.data.copy_(copied_weights)\n","                        if bias_flag:\n","                            conv1.bias.data.copy_(sub_layer.conv1.bias.data[out_indices])\n","                        new_block.conv1 = conv1\n","\n","                        # BN1 수정\n","                        bn1 = nn.BatchNorm2d(len(out_indices),\n","                                           sub_layer.bn1.eps,\n","                                           sub_layer.bn1.momentum,\n","                                           sub_layer.bn1.affine,\n","                                           sub_layer.bn1.track_running_stats)\n","\n","                        bn1.weight.data.copy_(sub_layer.bn1.weight.data[out_indices])\n","                        bn1.bias.data.copy_(sub_layer.bn1.bias.data[out_indices])\n","                        if sub_layer.bn1.track_running_stats:\n","                            bn1.running_mean.copy_(sub_layer.bn1.running_mean[out_indices])\n","                            bn1.running_var.copy_(sub_layer.bn1.running_var[out_indices])\n","                        new_block.bn1 = bn1\n","\n","                        # Conv2 수정 (입력 채널만 pruning)\n","                        if sub_layer.conv2.bias is None:\n","                            bias_flag = False\n","                        else:\n","                            bias_flag = True\n","\n","                        conv2 = nn.Conv2d(len(out_indices),\n","                                        sub_layer.conv2.out_channels,\n","                                        kernel_size=sub_layer.conv2.kernel_size,\n","                                        stride=sub_layer.conv2.stride,\n","                                        padding=sub_layer.conv2.padding,\n","                                        bias=bias_flag)\n","\n","                        copied_weights = sub_layer.conv2.weight.data[:, out_indices]\n","                        conv2.weight.data.copy_(copied_weights)\n","                        if bias_flag:\n","                            conv2.bias.data.copy_(sub_layer.conv2.bias.data)\n","                        new_block.conv2 = conv2\n","\n","                        # bn2는 원본 그대로 사용\n","                        new_block.bn2 = sub_layer.bn2\n","\n","                        conv_layer_count += 1\n","                        new_seq_layers.append(new_block)\n","                        prev_out_channels = sub_layer.conv2.out_channels\n","                    else:\n","                        new_seq_layers.append(sub_layer)\n","\n","                self.features.append(nn.Sequential(*new_seq_layers))\n","            else:\n","                if isinstance(layer, nn.Conv2d):\n","                    prev_out_channels = layer.out_channels\n","                elif isinstance(layer, nn.Linear):\n","                    new_linear = layer  # 원본 linear 레이어 사용\n","                self.features.append(layer)\n","            if i == 9:\n","                break\n","\n","    def forward(self, x):\n","        for layer in self.features:\n","            x = layer(x)\n","            if isinstance(layer, nn.AdaptiveAvgPool2d):\n","                x = torch.flatten(x, 1)\n","        return x\n"]},{"cell_type":"code","execution_count":218,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1732533156181,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"mmxpMUTrsVOr"},"outputs":[],"source":["#print(net)"]},{"cell_type":"code","execution_count":219,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6559,"status":"ok","timestamp":1732533162734,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"iN5nARxf0s6q","outputId":"566a7977-99c0-47bb-e122-fbdec88b719c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[before resnet decore] decore accuracy :  tensor(0.8606) , precision :  tensor(0.8600) , recall :  tensor(0.8606)\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# net = torch.load('./decore_resnet_best.pth')\n","# net = net.to(device)\n","# print(net)\n","\n","accuracy, precision, recall = get_metrics(device, net, test_loader)\n","net.train()\n","print ('[before resnet decore] decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)"]},{"cell_type":"code","execution_count":220,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5617,"status":"ok","timestamp":1732533168331,"user":{"displayName":"이지아 (챌린쟈)","userId":"02836985489279042441"},"user_tz":-540},"id":"SJ3yjZVH0s6r","outputId":"0776d551-53fb-4ef6-fb02-540dd09f2419"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer 0 active channels: 64.0 / (64,)\n","Layer 1 active channels: 64.0 / (64,)\n","Layer 2 active channels: 128.0 / (128,)\n","Layer 3 active channels: 128.0 / (128,)\n","Layer 4 active channels: 256.0 / (256,)\n","Layer 5 active channels: 256.0 / (256,)\n","Layer 6 active channels: 512.0 / (512,)\n","Layer 7 active channels: 512.0 / (512,)\n","Debugging Block 0:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 1:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 0:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 1:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 0:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 1:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 0:\n","정상임 ==> Bypass weights 일정함\n","Debugging Block 1:\n","정상임 ==> Bypass weights 일정함\n","ModifiedResNet(\n","  (features): ModuleList(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Identity()\n","    (4): Sequential(\n","      (0): NewBasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv_bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (1): NewBasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv_bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): NewBasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (conv_bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (1): NewBasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv_bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): NewBasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (conv_bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (1): NewBasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv_bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (7): Sequential(\n","      (0): NewBasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (conv_bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (1): NewBasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv_bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (9): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","[modified resnet decore] accuracy :  tensor(0.8616) , precision :  tensor(0.8610) , recall :  tensor(0.8616)\n"]}],"source":["# DECORE 모델에서 활성화된 채널 정보 가져오기\n","active_channels_list = []\n","for agent in net.agents:\n","    # 각 agent에서 활성화된 채널의 인덱스 가져오기 (1차원으로 추출)\n","    active_channels_list.append(agent.A[-1].detach().cpu().numpy())\n","\n","for i, active_channels in enumerate(active_channels_list):\n","    print(f\"Layer {i} active channels: {active_channels.sum().item()} / {active_channels.shape}\")\n","\n","\n","modified_resnet = ModifiedResNet(net.target_model.eval(), active_channels_list).to(device)\n","print(modified_resnet)\n","\n","accuracy, precision, recall = get_metrics(device, modified_resnet.eval(), test_loader)\n","print ('[modified resnet decore] accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04288712d09a45d88eb6f989cd285513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd8aef59a6da470baab5954d110db8f9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b8a588eda6d4edaab7ebacba9ceb918","value":1}},"29c98210939d4662ab597e8de2ef61f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69fd37a6b4854a5590db6777dbc2c66d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f436fc1aa364e068cbef5c859f57320","placeholder":"​","style":"IPY_MODEL_29c98210939d4662ab597e8de2ef61f5","value":"0.021 MB of 0.021 MB uploaded\r"}},"6f436fc1aa364e068cbef5c859f57320":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8a588eda6d4edaab7ebacba9ceb918":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7f8bbcc79784612a311b295353da9d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4bc954e7b604fffaf12c1522bdf312e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_69fd37a6b4854a5590db6777dbc2c66d","IPY_MODEL_04288712d09a45d88eb6f989cd285513"],"layout":"IPY_MODEL_a7f8bbcc79784612a311b295353da9d3"}},"dd8aef59a6da470baab5954d110db8f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LKhbwRUZWmTT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self,id,num_channels, on_off_rate = 1):\n",
        "        print(\"agent made\")\n",
        "        super().__init__()\n",
        "        self.on_off_rate = on_off_rate\n",
        "        self.register_parameter(f'S_{id}',nn.Parameter(torch.ones(num_channels) * 6.9))\n",
        "        _,S = next(self.named_parameters())\n",
        "        p = nn.Sigmoid()(S.expand(1,num_channels))\n",
        "        self.A = torch.bernoulli(p) # Actions\n",
        "        self.Pi = torch.Tensor(self.A.shape) # policy probability\n",
        "        self.Pi[self.A==1] = p[self.A==1]\n",
        "        self.Pi[self.A==0] = 1 - p[self.A==0]\n",
        "        self.R = torch.sum(abs(self.on_off_rate - self.A),dim=1) # Reward\n",
        "\n",
        "    def forward(self,I):\n",
        "        #print(\"agent forward\")\n",
        "\n",
        "        N,c,h,w = I.shape\n",
        "        _,S = next(self.named_parameters())\n",
        "\n",
        "        p = nn.Sigmoid()(S.expand(N,c))\n",
        "        if self.training:\n",
        "            self.A = torch.bernoulli(p)\n",
        "        else:\n",
        "            self.A = self.A.detach()\n",
        "        self.Pi = torch.Tensor(self.A.shape).to(S.device)\n",
        "        self.Pi[self.A==1] = p[self.A==1]\n",
        "        self.Pi[self.A==0] = 1 - p[self.A==0]\n",
        "        self.R = torch.sum(abs(self.on_off_rate - self.A),dim=1)\n",
        "        return I*self.A.view(N,c,1,1).contiguous().expand(N,c,h,w)\n",
        "\n",
        "    def count_active_channels(self):\n",
        "        \"\"\" 현재 활성화된 채널 수를 반환 \"\"\"\n",
        "\n",
        "        return torch.sum(self.A[-1]).item()\n",
        "\n",
        "    def count_all_channels(self):\n",
        "        \"\"\" 모든 채널 수를 반환 \"\"\"\n",
        "\n",
        "        return self.A[-1].numel()\n",
        "\n",
        "\n",
        "class DECOR(nn.Module):\n",
        "    def __init__(self, Model):\n",
        "        super().__init__()\n",
        "        self.target_model = Model\n",
        "        self.agents_list = [] # To keep track of all newly added agents\n",
        "        self.parse_model()\n",
        "        self.agents = nn.ModuleList(self.agents_list)\n",
        "\n",
        "    def parse_model(self):\n",
        "        \"\"\" This method parse the target model and append agents after the activation function of each convolution, the dictionary of the\n",
        "            target model should be defined for easy parsing \"\"\"\n",
        "        n = ''\n",
        "        modules = torch.nn.Sequential()\n",
        "        modules_list = [nn.Conv2d]\n",
        "        for i in self.target_model.state_dict().keys():\n",
        "            if n != i.split('.')[0]:\n",
        "                n = i.split('.')[0]\n",
        "                a = getattr(self.target_model,f'{n}')\n",
        "                if hasattr(a, '__iter__'):\n",
        "                    modules = torch.nn.Sequential()\n",
        "                    for idx in range(0,len(a)):\n",
        "                        if type(a[idx]) in modules_list and type(a[idx+1]) == nn.BatchNorm2d and type(a[idx+2]) == nn.ReLU:\n",
        "                            c = next(a[idx].parameters()).shape[0]\n",
        "                            # a[idx].requires_grad = False\n",
        "                            # a[idx+1].requires_grad = False\n",
        "                            # a[idx+2].requires_grad = False\n",
        "                            modules.add_module(f'{idx}',a[idx])\n",
        "                            modules.add_module(f'{idx+1}',a[idx+1])\n",
        "                            modules.add_module(f'{idx+2}',a[idx+2])\n",
        "                            agnt = Agent(len(self.agents_list),c)\n",
        "                            modules.add_module(f'Agent{len(self.agents_list)}',agnt)\n",
        "                            self.agents_list.append(agnt)\n",
        "                            idx += 4\n",
        "                        else:\n",
        "                            #TODO: frozen layer\n",
        "                            modules.add_module(f'{idx}',a[idx])\n",
        "\n",
        "                    a = modules\n",
        "                    setattr(self.target_model,f'{n}',a)\n",
        "\n",
        "    def forward(self,I):\n",
        "        return self.target_model(I)\n",
        "\n",
        "    def frozen_agent(self, is_fronzen=True):\n",
        "        for agent in self.agents_list:\n",
        "            for name, param in agent.named_parameters():\n",
        "                param.requires_grad = not is_fronzen\n",
        "\n",
        "    def count_total_active_channels(self):\n",
        "        \"\"\" 모든 에이전트에서 활성화된 채널 수의 총합을 반환 \"\"\"\n",
        "        total_active_channels = sum(agent.count_active_channels() for agent in self.agents)\n",
        "        return total_active_channels\n",
        "\n",
        "    def count_total_all_channels(self):\n",
        "        \"\"\" 모든 에이전트에서 모든 채널 수의 총합을 반환 \"\"\"\n",
        "        #print('agent총 수 : ', len(self.agents))\n",
        "        total_channels = sum(agent.count_all_channels() for agent in self.agents)\n",
        "        return total_channels\n",
        "\n",
        "    def print_active_channel(self):\n",
        "        \"\"\"각 에이전트의 활성화된 채널 수 출력\"\"\"\n",
        "        for index, agent in enumerate(self.agents, 1):\n",
        "            #print(f\"Agent {index} 상태 - A: {agent.A[0]}\")  # A 벡터 출력\n",
        "            active_channels = agent.count_active_channels()\n",
        "            print(f\"CNN Layer #{index} Active Channels: {active_channels} / All Channels: {agent.count_all_channels()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EEKLB8GqdBl3"
      },
      "outputs": [],
      "source": [
        "## custom loss\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, reward, penalty):\n",
        "        super().__init__()\n",
        "        self.reward = reward\n",
        "        self.penalty = penalty # How much to penalize incorrect drop\n",
        "\n",
        "    def forward(self, agents_list, y_predicted, target):\n",
        "        loss = 0.0\n",
        "        N,_ = y_predicted.size()\n",
        "        y_predicted = y_predicted.argmax(axis = 1)\n",
        "        t = torch.Tensor(y_predicted.shape).to(y_predicted.device)\n",
        "        t[y_predicted==target] = self.reward\n",
        "        t[y_predicted!=target] = self.penalty\n",
        "        # print(t)\n",
        "        for a in agents_list:\n",
        "            R = a.R*t\n",
        "            loss += torch.mean(torch.sum(torch.log(a.Pi),dim=1)* R)\n",
        "        return -loss # The aim is to maximize rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-nm89KeAWmTU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Matching active channel indices in the flattened output\n",
        "def get_flatten_indices(active_channels, height, width):\n",
        "    flattened_indices = []\n",
        "    for channel in active_channels:\n",
        "        # Calculate indices for each active channel, taking height * width for each\n",
        "        start_idx = int(channel) * height * width\n",
        "        end_idx = start_idx + height * width\n",
        "        flattened_indices.extend(range(start_idx, end_idx))\n",
        "    return flattened_indices\n",
        "\n",
        "class ModifiedVGG16(nn.Module):\n",
        "    def __init__(self, vgg16, active_channels_list):\n",
        "        super(ModifiedVGG16, self).__init__()\n",
        "        self.features = None\n",
        "        self.classifier = None\n",
        "        self.avgpool = None\n",
        "        self.active_channels_list = active_channels_list\n",
        "\n",
        "        conv_layer_count = 0\n",
        "        prev_out_channels = 3\n",
        "        vgg16.eval()\n",
        "\n",
        "        feature_class = True\n",
        "        for i, layer in enumerate(vgg16.children()):\n",
        "            print('i layer : ', type(layer))\n",
        "            if isinstance(layer, nn.Sequential) and feature_class:\n",
        "                new_seq_layers = []\n",
        "                # Sequential 내부의 Conv2d 레이어 탐색\n",
        "                j = 0\n",
        "                while j < len(layer):\n",
        "                    sub_layer = layer[j]\n",
        "                    if type(sub_layer) == nn.Conv2d and type(layer[j+1]) == nn.BatchNorm2d and type(layer[j+2]) == nn.ReLU:\n",
        "                        print(' j : ', j, type(sub_layer), ' conv2d?')\n",
        "                        #print(sub_layer)\n",
        "                        # 현재 레이어의 활성화된 출력 채널\n",
        "\n",
        "                        #agent_layer = layer[j+3].eval()\n",
        "\n",
        "\n",
        "                        active_out_channels = torch.tensor(self.active_channels_list[conv_layer_count], dtype=torch.bool)\n",
        "                            #agent_layer_active = torch.tensor(agent_layer.A, dtype=torch.bool)\n",
        "\n",
        "                            # assert torch.equal(active_out_channels.to(device),agent_layer_active[0]), \\\n",
        "                            #     f\"active mismatch: {active_out_channels} vs {agent_layer_active[0]}\"\n",
        "                        #print('active_out_channels ' , active_out_channels)\n",
        "\n",
        "                        # 이전 레이어의 활성화된 입력 채널\n",
        "                        if conv_layer_count > 0:\n",
        "                            active_in_channels = torch.tensor(self.active_channels_list[conv_layer_count - 1], dtype=torch.bool)\n",
        "                            #print('active_in_channels ',  active_in_channels)\n",
        "                        else:\n",
        "                            active_in_channels = torch.ones(prev_out_channels, dtype=torch.bool)\n",
        "                            #print('active_in_channels ',  active_in_channels)\n",
        "\n",
        "                        #print(\"bias : \", sub_layer.bias)\n",
        "\n",
        "\n",
        "                        new_conv = nn.Conv2d(in_channels=active_in_channels.sum().item(),\n",
        "                            out_channels=active_out_channels.sum().item(),\n",
        "                            kernel_size=sub_layer.kernel_size,\n",
        "                            stride=sub_layer.stride,\n",
        "                            padding=sub_layer.padding,\n",
        "                            dilation=sub_layer.dilation,\n",
        "                            groups=sub_layer.groups,\n",
        "                            bias=True)\n",
        "\n",
        "                        # 기존 가중치에서 활성화된 출력 채널에 해당하는 가중치만 가져오기\n",
        "                        out_indices = active_out_channels.nonzero().squeeze(1)\n",
        "                        in_indices = active_in_channels.nonzero().squeeze(1)\n",
        "                        # 가중치 복사\n",
        "                        #out_indices = active_out_channels.nonzero(as_tuple=True)[0]\n",
        "                        print('out_indices ',  out_indices)\n",
        "                        #in_indices = active_in_channels.nonzero(as_tuple=True)[0]\n",
        "                        print('in_indices ',  in_indices)\n",
        "                        new_bn = nn.BatchNorm2d(len(out_indices), eps=layer[j+1].eps, momentum=layer[j+1].momentum, affine=layer[j+1].affine, track_running_stats=layer[j+1].track_running_stats)\n",
        "                        # 활성화된 채널만 가져와서 새로운 레이어에 가중치 할당\n",
        "                        with torch.no_grad():\n",
        "                            # 가중치와 모ag양이 일치하는지 확인\n",
        "                            # copied_weights = sub_layer.weight.data[out_indices][:, in_indices]\n",
        "                            # assert copied_weights.shape == new_conv.weight.data.shape, \\\n",
        "                            #     f\"Weight shape mismatch: {copied_weights.shape} vs {new_conv.weight.data.shape}\"\n",
        "                            # new_conv.weight.data[:] = copied_weights.clone()\n",
        "                            # new_conv.bias.data[:] = sub_layer.bias.data[out_indices].clone()\n",
        "                            #new_conv.bias.data.copy_(sub_layer.bias.data[out_indices])\n",
        "\n",
        "                            copied_weights = sub_layer.weight.data[out_indices][:, in_indices]\n",
        "                            assert copied_weights.shape == new_conv.weight.data.shape, \\\n",
        "                                f\"Weight shape mismatch: {copied_weights.shape} vs {new_conv.weight.data.shape}\"\n",
        "                            new_conv.weight.data.copy_(copied_weights)\n",
        "                            new_conv.bias.data.copy_(sub_layer.bias.data[out_indices])\n",
        "                            new_seq_layers.append(new_conv)\n",
        "                            conv_layer_count += 1\n",
        "                            prev_out_channels = int(sum(active_out_channels))\n",
        "                            #BatchNorm2D넣기\n",
        "\n",
        "\n",
        "                            new_bn.weight.data.copy_(layer[j+1].weight.data[out_indices])\n",
        "                            new_bn.weight.requires_grad = layer[j+1].weight.requires_grad\n",
        "                            new_bn.bias.data.copy_(layer[j+1].bias.data[out_indices])\n",
        "                            new_bn.bias.requires_grad = layer[j+1].bias.requires_grad\n",
        "                            if layer[j+1].track_running_stats:\n",
        "                                new_bn.running_mean.copy_(layer[j+1].running_mean[out_indices])\n",
        "                                new_bn.running_var.copy_(layer[j+1].running_var[out_indices])\n",
        "                                #new_bn.running_var.requires_grad = layer[j+1].running_var[out_indices].requrires_grad\n",
        "                            new_seq_layers.append(new_bn)\n",
        "                            #Relu 넣기 ReLU\n",
        "                            #relu = nn.ReLU(inplace=True)\n",
        "                            new_seq_layers.append(layer[j+2])\n",
        "\n",
        "\n",
        "                        # if j == 52:\n",
        "                        #    new_seq_layers.append(agent_layer)\n",
        "                        j += 4\n",
        "                    elif type(sub_layer) == nn.MaxPool2d: # MaxPool2d\n",
        "                        new_seq_layers.append(sub_layer)\n",
        "                        j += 1\n",
        "                    else:\n",
        "                        j += 1\n",
        "                self.features = nn.Sequential(*new_seq_layers)\n",
        "                feature_class = False\n",
        "\n",
        "            else:\n",
        "                #(avgpool): AdaptiveAvgPool2d(output_size=(7, 7), (classifier): Sequential,\n",
        "                if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
        "                    self.avgpool = layer\n",
        "                else:\n",
        "                    flattened_size = 1 * 1 * prev_out_channels\n",
        "\n",
        "                    active_channel_index = np.where(self.active_channels_list[conv_layer_count - 1] == 1)[0]\n",
        "                    assert len(active_channel_index) == sum(self.active_channels_list[conv_layer_count - 1]), \\\n",
        "                    \"Mismatch in active channels\"\n",
        "                    flatten_indices = get_flatten_indices(active_channel_index, 1, 1)\n",
        "\n",
        "                    linear = nn.Linear(flattened_size, 4096)\n",
        "                    with torch.no_grad():\n",
        "                        # 가중치 복사 및 모양 일치 확인\n",
        "                        copied_weights = layer[0].weight.data[:, flatten_indices]\n",
        "                        assert copied_weights.shape == linear.weight.data.shape, \\\n",
        "                            f\"Weight shape mismatch: {copied_weights.shape} vs {linear.weight.data.shape}\"\n",
        "                        linear.weight.data.copy_(copied_weights)\n",
        "                        linear.bias.data.copy_(layer[0].bias.data)\n",
        "                    self.classifier=nn.Sequential(\n",
        "                        linear,\n",
        "                        #layer[0],\n",
        "                        layer[1],\n",
        "                        layer[2],\n",
        "                        layer[3],\n",
        "                        layer[4],\n",
        "                        layer[5],\n",
        "                        layer[6]\n",
        "                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Convolutional layers\n",
        "        x = self.avgpool(x)  # Adaptive average pooling\n",
        "        x = torch.flatten(x, -3)  # Flatten for the classifier\n",
        "        x = self.classifier(x)  # Fully connected layers\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woWxdqctWmTU",
        "outputId": "81a2f4b7-2fec-4398-dbfb-7b42d7d41474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./__data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./__data/cifar-10-python.tar.gz to ./__data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./__data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./__data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "TnvMS7vDWmTU",
        "outputId": "57b6ce27-5179-4eba-97d9-52221eb25fe0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 이미지와 라벨 하나 가져오기\n",
        "image, label = testset[0]\n",
        "\n",
        "# 이미지 원래 범위로 복원 (정규화 해제)\n",
        "image = image * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "\n",
        "# 이미지 출력\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(classes[label])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR0SPZWEWmTV",
        "outputId": "eaad0e77-f912-45a6-f979-8a83a312e990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall\n",
        "\n",
        "metric_collection = MetricCollection([\n",
        "    MulticlassAccuracy(num_classes=10, average=\"micro\"),\n",
        "    MulticlassPrecision(num_classes=10, average=\"macro\"),\n",
        "    MulticlassRecall(num_classes=10, average=\"macro\")\n",
        "])\n",
        "\n",
        "def get_metrics(device, model, test_loader):\n",
        "  with torch.no_grad():\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    for x, y in test_loader:\n",
        "      x = x.to(device)\n",
        "      y = y\n",
        "      outputs = model(x)\n",
        "      _, predicted = outputs.max(1)\n",
        "      metric_collection.update(predicted.cpu(), y)\n",
        "\n",
        "  metric_output = metric_collection.compute()\n",
        "\n",
        "  #print(metric_output)\n",
        "\n",
        "  acc, precision, recall = metric_output['MulticlassAccuracy'], metric_output['MulticlassPrecision'], metric_output['MulticlassRecall']\n",
        "  metric_collection.reset()\n",
        "\n",
        "  return acc, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efN77OhHwLYB",
        "outputId": "631b4580-8e07-4e6d-b6fd-85d5a1cf2f4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 75.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Import a pre-trained VGG16 and replace classification layer with new layer that has number of classes in CIFAR-10\n",
        "vgg16 = models.vgg16_bn(weights='IMAGENET1K_V1')\n",
        "input_lastLayer = vgg16.classifier[6].in_features\n",
        "vgg16.classifier[6] = nn.Linear(input_lastLayer,10)\n",
        "vgg16.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "vgg16.classifier[0] = nn.Linear(512, 4096)\n",
        "model = vgg16.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXuY85UlJ6xn",
        "outputId": "b7194dea-73f6-4c6b-b07c-7387c1a789b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "# import torch.optim as optim\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIYcGcsKb7sT",
        "outputId": "cae7ed10-934a-4295-fbb6-e3705b9f514f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15574564864, 15835660288)\n",
            "cuda:0\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "agent made\n",
            "penalty: -10\n",
            "(15574564864, 15835660288)\n",
            "decore training start\n",
            "epoch 1/1, step: 250/500: loss = -86809.42969, acc = 9.65%, channel = 99.91%, lr =0.01 \n",
            "epoch 1/1, step: 500/500: loss = -143565.76562, acc = 9.61%, channel = 99.93%, lr =0.01 \n",
            "Layer 0 - Agent Active Channels: 64.0, List Active Channels: 64.0\n",
            "Layer 1 - Agent Active Channels: 64.0, List Active Channels: 64.0\n",
            "Layer 2 - Agent Active Channels: 128.0, List Active Channels: 128.0\n",
            "Layer 3 - Agent Active Channels: 128.0, List Active Channels: 128.0\n",
            "Layer 4 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 5 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 6 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 7 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 8 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 9 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 10 - Agent Active Channels: 511.0, List Active Channels: 511.0\n",
            "Layer 11 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 12 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 0 active channels: 64.0 / (64,)\n",
            "Layer 1 active channels: 64.0 / (64,)\n",
            "Layer 2 active channels: 128.0 / (128,)\n",
            "Layer 3 active channels: 128.0 / (128,)\n",
            "Layer 4 active channels: 256.0 / (256,)\n",
            "Layer 5 active channels: 256.0 / (256,)\n",
            "Layer 6 active channels: 256.0 / (256,)\n",
            "Layer 7 active channels: 512.0 / (512,)\n",
            "Layer 8 active channels: 512.0 / (512,)\n",
            "Layer 9 active channels: 512.0 / (512,)\n",
            "Layer 10 active channels: 511.0 / (512,)\n",
            "Layer 11 active channels: 512.0 / (512,)\n",
            "Layer 12 active channels: 512.0 / (512,)\n",
            "decore 모델 학습 시간: 0분 32초\n",
            "총 채널 수 :  4224\n",
            "활성화된 채널 수 :  4223.0\n",
            "decore accuracy :  tensor(0.0926) , precision :  tensor(0.0513) , recall :  tensor(0.0926)\n",
            "CNN Layer #1 Active Channels: 64.0 / All Channels: 64\n",
            "CNN Layer #2 Active Channels: 64.0 / All Channels: 64\n",
            "CNN Layer #3 Active Channels: 128.0 / All Channels: 128\n",
            "CNN Layer #4 Active Channels: 128.0 / All Channels: 128\n",
            "CNN Layer #5 Active Channels: 256.0 / All Channels: 256\n",
            "CNN Layer #6 Active Channels: 256.0 / All Channels: 256\n",
            "CNN Layer #7 Active Channels: 256.0 / All Channels: 256\n",
            "CNN Layer #8 Active Channels: 512.0 / All Channels: 512\n",
            "CNN Layer #9 Active Channels: 512.0 / All Channels: 512\n",
            "CNN Layer #10 Active Channels: 512.0 / All Channels: 512\n",
            "CNN Layer #11 Active Channels: 511.0 / All Channels: 512\n",
            "CNN Layer #12 Active Channels: 512.0 / All Channels: 512\n",
            "CNN Layer #13 Active Channels: 512.0 / All Channels: 512\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# from google.colab import drive\n",
        "# import torch.optim as optim\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# model = torch.load('/content/drive/MyDrive/decore_model/vgg_model_bn.pth')\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.mem_get_info())\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "num_epochs = 1\n",
        "lr = 0.01\n",
        "net = DECOR(model)\n",
        "net = net.to(device)\n",
        "\n",
        "id = 0\n",
        "param = []\n",
        "\n",
        "for n,p in net.named_parameters():\n",
        "    if n.endswith(f\".S_{id}\") and p.requires_grad:\n",
        "        param.append(p)\n",
        "        id += 1\n",
        "\n",
        "#optimizer = Adam(param, lr=lr)\n",
        "\n",
        "optimizer = optim.Adam(net.agents.parameters(), lr=lr) # 이거로 변경해보기\n",
        "#optimizer = torch.optim.SGD(net.agents.parameters(), lr = lr, momentum=0.9,weight_decay=5e-4)\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr = lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = torch.optim.SGD(net.agents.parameters(), lr = lr)\n",
        "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,40,60,80], gamma=0.5)\n",
        "criterion_decore = CustomLoss(1, -10)\n",
        "print(f'penalty: {criterion_decore.penalty}')\n",
        "\n",
        "\n",
        "\n",
        "n_total_step = len(trainloader)\n",
        "\n",
        "import gc\n",
        "import time\n",
        "\n",
        "torch.set_printoptions(edgeitems=torch.inf)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.mem_get_info())\n",
        "print(\"decore training start\")\n",
        "# 학습 시작 시간 기록\n",
        "start_time = time.time()\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_corrects = 0 # Accuracy calculation\n",
        "    total_samples = 0 # Accuracy calculation\n",
        "    total_loss = 0\n",
        "    total_all_channel = 0\n",
        "    total_active_channel = 0\n",
        "    #train_loss = 0.0\n",
        "    for i, (imgs , labels) in enumerate(trainloader):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels_hat = net.target_model(imgs)\n",
        "        loss_value = criterion_decore(net.agents_list,labels_hat, labels)\n",
        "\n",
        "        _, preds = torch.max(labels_hat, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        total_loss += loss_value\n",
        "        total_all_channel += net.count_total_all_channels()\n",
        "        total_active_channel += net.count_total_active_channels()\n",
        "\n",
        "        if (i + 1) % 250 == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {total_loss:.5f}, acc = {(100 * running_corrects / total_samples):.2f}%, channel = { total_active_channel / total_all_channel * 100:.2f}%, lr ={optimizer.param_groups[0][\"lr\"]} ')\n",
        "            if 0:\n",
        "                for agent in net.agents_list:\n",
        "                    rounded_Pi = torch.round(agent.Pi * 10000) / 10000\n",
        "                    unique_values, counts = torch.unique(rounded_Pi, return_counts=True)\n",
        "                    #print(agent.Pi[agent.Pi != 0.999])\n",
        "                    for value, count in zip(unique_values, counts):\n",
        "                        print(f\"Value: {value.item():.5f}, Count: {count.item()}\")\n",
        "                    break\n",
        "                total_loss = 0\n",
        "\n",
        "        #train_loss += loss_value.detach().cpu().item() / len(train_loader)\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "# DECORE 모델에서 활성화된 채널 정보 가져오기\n",
        "net.eval() # 베르누이 확률 분포에 의해 eval모드가 아니면 값이 변경됨\n",
        "\n",
        "\n",
        "active_channels_list = []\n",
        "for agent in net.agents:\n",
        "    agent.eval()\n",
        "    # 각 agent에서 활성화된 채널의 인덱스 가져오기 (1차원으로 추출)\n",
        "    active_channels_list.append(agent.A[-1].detach().cpu().numpy())\n",
        "\n",
        "for i, (agent, active_channels) in enumerate(zip(net.agents, active_channels_list)):\n",
        "    print(f\"Layer {i} - Agent Active Channels: {agent.count_active_channels()}, \"\n",
        "          f\"List Active Channels: {sum(active_channels)}\")\n",
        "\n",
        "for i, active_channels in enumerate(active_channels_list):\n",
        "    print(f\"Layer {i} active channels: {sum(active_channels)} / {active_channels.shape}\")\n",
        "\n",
        "# 학습 종료 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 총 학습 시간 계산\n",
        "total_time = end_time - start_time\n",
        "print(f\"decore 모델 학습 시간: {total_time // 60:.0f}분 {total_time % 60:.0f}초\")\n",
        "print(\"총 채널 수 : \", net.eval().count_total_all_channels())\n",
        "print(\"활성화된 채널 수 : \",net.eval().count_total_active_channels())\n",
        "\n",
        "\n",
        "accuracy, precision, recall = get_metrics(device, net.target_model.eval(), testloader)\n",
        "\n",
        "print ('decore accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n",
        "\n",
        "\n",
        "# DECORE 모델에서 활성화된 채널 정보 가져오기\n",
        "net.eval().print_active_channel()\n",
        "\n",
        "# Google Drive에 모델 가중치 저장\n",
        "torch.save(net.state_dict(), '/content/drive/MyDrive/decore_model/decore_vgg_bn_model_weights.pth')\n",
        "\n",
        "# 전체 모델 저장\n",
        "torch.save(net, '/content/drive/MyDrive/decore_model/decore_vgg_bn_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h72_PtEmJsUi",
        "outputId": "28f02d01-2ad8-48bb-90e8-c1947be96379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (Agent0): Agent()\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (Agent1): Agent()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (Agent2): Agent()\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (Agent3): Agent()\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (Agent4): Agent()\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (Agent5): Agent()\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (Agent6): Agent()\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (Agent7): Agent()\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (Agent8): Agent()\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (Agent9): Agent()\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (Agent10): Agent()\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (Agent11): Agent()\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (Agent12): Agent()\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj1sD1uRJt_a",
        "outputId": "a943b369-f54b-485e-c915-e4315e126bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (Agent0): Agent()\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (Agent1): Agent()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (Agent2): Agent()\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (Agent3): Agent()\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (Agent4): Agent()\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (Agent5): Agent()\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (Agent6): Agent()\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (Agent7): Agent()\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (Agent8): Agent()\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (Agent9): Agent()\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (Agent10): Agent()\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (Agent11): Agent()\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (Agent12): Agent()\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(net.target_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1_-Twhn1c0G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RYZe5uYVWVYP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ModifiedVGG16Test(nn.Module):\n",
        "    def __init__(self, vgg16, active_channels_list):\n",
        "        super(ModifiedVGG16Test, self).__init__()\n",
        "        self.features = None\n",
        "        self.classifier = None\n",
        "        self.avgpool = None\n",
        "        self.active_channels_list = active_channels_list\n",
        "\n",
        "        conv_layer_count = 0\n",
        "        prev_out_channels = 3\n",
        "        vgg16.eval()\n",
        "\n",
        "        feature_class = True\n",
        "        for i, layer in enumerate(vgg16.children()):\n",
        "            #print('i layer : ', type(layer))\n",
        "            if isinstance(layer, nn.Sequential) and feature_class:\n",
        "                new_seq_layers = []\n",
        "                # Sequential 내부의 Conv2d 레이어 탐색\n",
        "                j = 0\n",
        "                while j < len(layer):\n",
        "                    sub_layer = layer[j]\n",
        "                    if type(sub_layer) == nn.Conv2d and type(layer[j+1]) == nn.BatchNorm2d and type(layer[j+2]) == nn.ReLU:\n",
        "                        #print(type(sub_layer), ' conv2d?')\n",
        "                        #print(sub_layer)\n",
        "                        # 현재 레이어의 활성화된 출력 채널\n",
        "                        new_conv = sub_layer\n",
        "                        new_seq_layers.append(new_conv)\n",
        "\n",
        "\n",
        "                        new_bn = nn.BatchNorm2d(layer[j+1].num_features, eps=layer[j+1].eps, momentum=layer[j+1].momentum, affine=layer[j+1].affine, track_running_stats=layer[j+1].track_running_stats)\n",
        "                        new_bn.weight.data.copy_(layer[j+1].weight.data)\n",
        "                        new_bn.bias.data.copy_(layer[j+1].bias.data)\n",
        "                        if layer[j+1].track_running_stats:\n",
        "                            new_bn.running_mean.copy_(layer[j+1].running_mean)\n",
        "                            new_bn.running_var.copy_(layer[j+1].running_var)\n",
        "                        new_seq_layers.append(new_bn)\n",
        "\n",
        "                        # new_bn = layer[j+1]\n",
        "                        # new_seq_layers.append(new_bn)\n",
        "\n",
        "                        #Relu 넣기 ReLU\n",
        "                        relu = nn.ReLU(inplace=True)\n",
        "                        new_seq_layers.append(relu)\n",
        "\n",
        "                        j += 3\n",
        "                    elif type(sub_layer) == nn.MaxPool2d: # MaxPool2d\n",
        "                        new_seq_layers.append(sub_layer)\n",
        "                        j += 1\n",
        "                    else:# Agent\n",
        "                        new_seq_layers.append(sub_layer)\n",
        "                        j += 1\n",
        "                self.features = nn.Sequential(*new_seq_layers)\n",
        "                feature_class = False\n",
        "\n",
        "                #print(\"conv_layer_count \", conv_layer_count)\n",
        "            else:\n",
        "                #(avgpool): AdaptiveAvgPool2d(output_size=(7, 7), (classifier): Sequential,\n",
        "                if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
        "                    self.avgpool = layer\n",
        "                else:\n",
        "\n",
        "                    self.classifier=layer\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Convolutional layers\n",
        "        x = self.avgpool(x)  # Adaptive average pooling\n",
        "        x = torch.flatten(x, -3)  # Flatten for the classifier\n",
        "        x = self.classifier(x)  # Fully connected layers\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5nrb0rVlWmTV"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "\n",
        "def pruning_vgg_model(target_path):\n",
        "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\tprint(device)\n",
        "\n",
        "\t#net = torch.load(\"decore_vgg_model.pth\")\n",
        "\tnet = torch.load(\"/content/drive/MyDrive/decore_model/decore_vgg_bn_model.pth\")\n",
        "\tnet = net.to(device)\n",
        "\t#print(model)\n",
        "\tnet.eval()\n",
        "\tnet.target_model.eval()\n",
        "\n",
        "\tactive_channels_list = []\n",
        "\tfor agent in net.agents:\n",
        "\t\tagent.eval()\n",
        "\t\t# 각 agent에서 활성화된 채널의 인덱스 가져오기 (1차원으로 추출)\n",
        "\t\tactive_channels_list.append(agent.A[-1].detach().cpu().numpy())\n",
        "\n",
        "\tfor i, (agent, active_channels) in enumerate(zip(net.agents, active_channels_list)):\n",
        "\t\tprint(f\"Layer {i} - Agent Active Channels: {agent.count_active_channels()}, \"\n",
        "\t\t\tf\"List Active Channels: {sum(active_channels)}\")\n",
        "\n",
        "\tfor i, active_channels in enumerate(active_channels_list):\n",
        "\t\tprint(f\"Layer {i} active channels: {sum(active_channels)} / {active_channels.shape}\")\n",
        "\n",
        "\tprint(net.target_model.eval())\n",
        "\n",
        "\t# 채널 수 줄인 VGG\n",
        "\tnet.frozen_agent()\n",
        "\tmodified_vgg = ModifiedVGG16(net.target_model.eval(), active_channels_list)\n",
        "\tmodified_vgg = modified_vgg.to(device)\n",
        "\n",
        "\tmodified_vgg_test = ModifiedVGG16Test(net.target_model.eval(), active_channels_list)\n",
        "\tmodified_vgg_test = modified_vgg_test.to(device)\n",
        "\n",
        "\taccuracy, precision, recall = get_metrics(device, modified_vgg.eval(), testloader)\n",
        "\tprint ('modified_vgg accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n",
        "\n",
        "\taccuracy, precision, recall = get_metrics(device, modified_vgg_test.eval(), testloader)\n",
        "\tprint ('modified_vgg_test accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n",
        "\n",
        "\t# fine tuning\n",
        "\t# Import a pre-trained VGG16 and replace classification layer with new layer that has number of classes in CIFAR-10\n",
        "\tvgg16_bn = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\tinput_lastLayer = vgg16_bn.classifier[6].in_features\n",
        "\tvgg16_bn.classifier[6] = nn.Linear(input_lastLayer,10)\n",
        "\tvgg16_bn.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\tvgg16_bn.classifier[0] = nn.Linear(512, 4096)\n",
        "\n",
        "\tmodel = vgg16_bn.to(device)\n",
        "\n",
        "\tdef get_model_size(model):\n",
        "\t\ttotal_size = 0\n",
        "\t\tfor param in model.parameters():\n",
        "\t\t\t# 각 파라미터의 크기 계산: 요소 수 * 데이터 타입의 바이트 수 (float32 = 4 bytes)\n",
        "\t\t\ttotal_size += param.nelement() * param.element_size()\n",
        "\n",
        "\t\t# 메가바이트(MB)로 변환\n",
        "\t\ttotal_size_mb = total_size / (1024 ** 2)\n",
        "\t\treturn total_size_mb\n",
        "\n",
        "\t# ModifiedVGG 모델의 용량 계산\n",
        "\tmodel_size = get_model_size(modified_vgg)\n",
        "\n",
        "\t# VGG16 모델의 용량 계산\n",
        "\tmodel_size_origin = get_model_size(model)\n",
        "\n",
        "\n",
        "\tprint(f\"기존 모델의 용량: {model_size_origin:.2f} MB\")\n",
        "\tprint(f\"ModifiedVGG 모델의 용량: {model_size:.2f} MB\")\n",
        "\n",
        "\n",
        "\taccuracy, precision, recall = get_metrics(device, net.target_model.eval(), testloader)\n",
        "\n",
        "\tprint ('decore.target model accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)\n",
        "\n",
        "\t# Google Drive에 모델 가중치 저장\n",
        "\ttorch.save(modified_vgg.state_dict(), 'pruned_vgg.pth')\n",
        "\n",
        "\t# 전체 모델 저장\n",
        "\ttorch.save(modified_vgg, target_path)\n",
        "\n",
        "\tprint(modified_vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEXczGdNWmTW",
        "outputId": "03b7e1a4-4613-47f6-8399-ed74dedff656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-e04cad64505a>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net = torch.load(\"/content/drive/MyDrive/decore_model/decore_vgg_bn_model.pth\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0 - Agent Active Channels: 64.0, List Active Channels: 64.0\n",
            "Layer 1 - Agent Active Channels: 64.0, List Active Channels: 64.0\n",
            "Layer 2 - Agent Active Channels: 128.0, List Active Channels: 128.0\n",
            "Layer 3 - Agent Active Channels: 128.0, List Active Channels: 128.0\n",
            "Layer 4 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 5 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 6 - Agent Active Channels: 256.0, List Active Channels: 256.0\n",
            "Layer 7 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 8 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 9 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 10 - Agent Active Channels: 511.0, List Active Channels: 511.0\n",
            "Layer 11 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 12 - Agent Active Channels: 512.0, List Active Channels: 512.0\n",
            "Layer 0 active channels: 64.0 / (64,)\n",
            "Layer 1 active channels: 64.0 / (64,)\n",
            "Layer 2 active channels: 128.0 / (128,)\n",
            "Layer 3 active channels: 128.0 / (128,)\n",
            "Layer 4 active channels: 256.0 / (256,)\n",
            "Layer 5 active channels: 256.0 / (256,)\n",
            "Layer 6 active channels: 256.0 / (256,)\n",
            "Layer 7 active channels: 512.0 / (512,)\n",
            "Layer 8 active channels: 512.0 / (512,)\n",
            "Layer 9 active channels: 512.0 / (512,)\n",
            "Layer 10 active channels: 511.0 / (512,)\n",
            "Layer 11 active channels: 512.0 / (512,)\n",
            "Layer 12 active channels: 512.0 / (512,)\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (Agent0): Agent()\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (Agent1): Agent()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (Agent2): Agent()\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (Agent3): Agent()\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (Agent4): Agent()\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (Agent5): Agent()\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (Agent6): Agent()\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (Agent7): Agent()\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (Agent8): Agent()\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (Agent9): Agent()\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (Agent10): Agent()\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (Agent11): Agent()\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (Agent12): Agent()\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "i layer :  <class 'torch.nn.modules.container.Sequential'>\n",
            " j :  0 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
            "in_indices  tensor([0, 1, 2])\n",
            " j :  4 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
            "in_indices  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
            " j :  9 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127])\n",
            "in_indices  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
            " j :  13 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127])\n",
            " j :  18 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127])\n",
            " j :  22 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            " j :  26 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            " j :  31 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255])\n",
            " j :  35 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            " j :  39 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            " j :  44 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
            "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
            "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
            "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
            "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
            "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
            "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
            "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
            "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
            "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
            "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
            "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
            "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
            "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
            "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
            "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
            "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
            "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
            "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
            "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
            "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
            "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
            "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
            "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
            "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
            "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
            "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
            "        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
            "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
            "        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
            "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
            "        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
            "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
            "        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
            "        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
            "        505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            " j :  48 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
            "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
            "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
            "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
            "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
            "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
            "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
            "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
            "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
            "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
            "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
            "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
            "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
            "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
            "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
            "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
            "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
            "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
            "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
            "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
            "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
            "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
            "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
            "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
            "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
            "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
            "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
            "        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
            "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
            "        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
            "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
            "        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
            "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
            "        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
            "        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
            "        505, 506, 507, 508, 509, 510, 511])\n",
            " j :  52 <class 'torch.nn.modules.conv.Conv2d'>  conv2d?\n",
            "out_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "in_indices  tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n",
            "i layer :  <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
            "i layer :  <class 'torch.nn.modules.container.Sequential'>\n",
            "modified_vgg accuracy :  tensor(0.0912) , precision :  tensor(0.0559) , recall :  tensor(0.0912)\n",
            "modified_vgg_test accuracy :  tensor(0.0926) , precision :  tensor(0.0513) , recall :  tensor(0.0926)\n",
            "기존 모델의 용량: 512.32 MB\n",
            "ModifiedResNet 모델의 용량: 128.32 MB\n",
            "decore.target model accuracy :  tensor(0.0926) , precision :  tensor(0.0513) , recall :  tensor(0.0926)\n",
            "ModifiedVGG16(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 511, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(511, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): Agent()\n",
            "    (44): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "ModifiedResNet 모델의 용량: 128.32 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-32-95f78648d633>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vgg_pruned = torch.load(prune_path)\n"
          ]
        }
      ],
      "source": [
        "def get_model_size(model):\n",
        "    total_size = 0\n",
        "    for param in model.parameters():\n",
        "        # 각 파라미터의 크기 계산: 요소 수 * 데이터 타입의 바이트 수 (float32 = 4 bytes)\n",
        "        total_size += param.nelement() * param.element_size()\n",
        "\n",
        "    # 메가바이트(MB)로 변환\n",
        "    total_size_mb = total_size / (1024 ** 2)\n",
        "    return total_size_mb\n",
        "\n",
        "prune_path = \"pruned_vgg_model.pth\"\n",
        "# try:\n",
        "# \tvgg_pruned = torch.load(prune_path)\n",
        "# except FileNotFoundError:\n",
        "pruning_vgg_model(prune_path)\n",
        "vgg_pruned = torch.load(prune_path)\n",
        "\n",
        "vgg_pruned = vgg_pruned.to('cuda')\n",
        "# print(vgg_pruned)\n",
        "vgg_pruned.eval()\n",
        "\n",
        "# ModifiedVGG 모델의 용량 계산\n",
        "model_size = get_model_size(vgg_pruned)\n",
        "\n",
        "print(f\"ModifiedResNet 모델의 용량: {model_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58SshFtPWmTW",
        "outputId": "f0535df0-e28c-466e-de8b-fc10c27128a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model accuracy :  tensor(0.8301) , precision :  tensor(0.8300) , recall :  tensor(0.8301)\n"
          ]
        }
      ],
      "source": [
        "accuracy, precision, recall = get_metrics('cuda', vgg_pruned, testloader)\n",
        "print ('model accuracy : ', accuracy, ', precision : ', precision, ', recall : ', recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbpvyNbGWmTW",
        "outputId": "7a121738-c396-4e86-c695-a203af0fb1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pred: cat\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\timg, label = testset[0]\n",
        "\timg = img.to('cuda')\n",
        "\tout = vgg_pruned(img)\n",
        "\t_, pred = out.max(0)\n",
        "\tprint(f'pred: {classes[pred]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
